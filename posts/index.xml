<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Floris den Hengst&#39;s Homepage</title>
    <link>https://florisdenhengst.github.io/posts/</link>
    <description>Recent content in Posts on Floris den Hengst&#39;s Homepage</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Jun 2022 14:49:38 +0200</lastBuildDate><atom:link href="https://florisdenhengst.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Paper on strategic workforkforce planning with DRL at LOD</title>
      <link>https://florisdenhengst.github.io/posts/rl-for-swp-lod/</link>
      <pubDate>Tue, 14 Jun 2022 14:49:38 +0200</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/rl-for-swp-lod/</guid>
      <description>A paper on Deep Reinforcement Learning (DRL) for strategic workforce planning co-authored with Yannick Smit, Sandjai Bhulai and Ehsan Mehdad is accepted as a long paper at LOD.
In this paper, we model strategic workforce planning as a stochastic nonlinear optimization problem, learn a generative model from data and use it as a simulator in a simulation-optimization approach.
We show that the DRL approach enables optimizing an organizations&#39; strategic workforce goals directly.</description>
    </item>
    
    <item>
      <title>Option Machines paper accepted at IJCAI</title>
      <link>https://florisdenhengst.github.io/posts/option-machines-ijcai/</link>
      <pubDate>Sat, 21 May 2022 14:44:12 +0200</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/option-machines-ijcai/</guid>
      <description>The paper Reinforcement Learning with Option Machines co-authored by me, Vincent Fran√ßois-Lavet, Mark Hoogendoorn and Frank van Harmelen is accepted as a long presentation (~3% acceptance rate) at IJCAI.
Stay tuned for details!</description>
    </item>
    
    <item>
      <title>Safe and Efficient Reinforcement Learning with Planning for Potential</title>
      <link>https://florisdenhengst.github.io/posts/planning-for-potential/</link>
      <pubDate>Thu, 24 Mar 2022 10:38:24 +0100</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/planning-for-potential/</guid>
      <description>Reinforcement Learning has proven to be capable of outperforming humans on various tasks by interacting with and experimenting some environment. This makes it one of the most interesting and promising AI solutions to problems that require complex behaviors which we are unable to fully define upfront but can assign an objective score to.
In many settings of interest such as in healthcare and finance, we&amp;rsquo;ll want to enforce safety constraints on any system of interest.</description>
    </item>
    
    <item>
      <title>Reinforcement Learning for Real Life Virtual Conference</title>
      <link>https://florisdenhengst.github.io/posts/rl-at-workshop/</link>
      <pubDate>Thu, 25 Jun 2020 21:14:18 +0200</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/rl-at-workshop/</guid>
      <description>Upcoming weekend will be a virtual conference on reinforcement learning (RL) in real-life.
It is of great interest to me primarily due to its program but also due to its organisation.
The virtual conference consists of two panel sessions and a virtual `poster&#39; session with pre-recorded videos. There is a slack workspace for discussion, and questions to the panelists can be submitted up-front. Poster presenters are to host their own video channel.</description>
    </item>
    
    <item>
      <title>Reinforcement Learning for Personalization: A Survey</title>
      <link>https://florisdenhengst.github.io/posts/rl-for-pers-survey/</link>
      <pubDate>Thu, 09 Apr 2020 09:22:30 +0100</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/rl-for-pers-survey/</guid>
      <description>Update 2020-06-27: this paper was presented as a `poster&#39;(video) at the RL for Real Life virtual Conference. Read more here.
Reinforcement learning (RL) is becoming an increasingly popular tool to tackle hairy problems using data. A nice example of such a hairy problem is personalization. Personalization refers to a task central to many applications of data science and machine learning: to change a system so that its personal relevance to an individual or category of individuals is increased.</description>
    </item>
    
    <item>
      <title>Collecting User Satisfaction Ratings for Dialogue Systems</title>
      <link>https://florisdenhengst.github.io/posts/high-quality-ratings/</link>
      <pubDate>Fri, 20 Mar 2020 11:35:13 +0100</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/high-quality-ratings/</guid>
      <description>Mickey, a master student I was co-supervising, recently published a paper based on his thesis work in ACM&amp;rsquo;s Conference for Human Information Interaction and Retrieval. His work focused on interfaces for collecting high-quality user satisfaction ratings for dialogue systems.
User satisfaction is an important indicator in the design, evaluation and adaptation of dialogue systems. Establishing user satisfaction ratings for conversation-based systems, however, remains challenging. User questionnaires may yield biased results and typically have low response rates (~1%).</description>
    </item>
    
    <item>
      <title>Featured in Villamedia Blog</title>
      <link>https://florisdenhengst.github.io/posts/villamedia-blog/</link>
      <pubDate>Wed, 18 Dec 2019 14:39:21 +0100</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/villamedia-blog/</guid>
      <description>Today, my work is &amp;ndash; very briefly &amp;ndash; mentioned in a post by Nick Kivits.
The post is part of a blog series on innovation in the media and it is hosted on Villamedia, a Dutch website on journalism.</description>
    </item>
    
    <item>
      <title>Personalized Dialogue Management</title>
      <link>https://florisdenhengst.github.io/posts/personalized-dm/</link>
      <pubDate>Mon, 21 Oct 2019 10:31:06 +0200</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/personalized-dm/</guid>
      <description>In a previous post I explained how reinforcement learning (RL) can be used to make chatbots better from its experience with users. RL allows chatbots to learn what to say by interacting with users. This allows for chatbots to tailor their behavior to preferences of groups of individuals, e.g. to personalize the interaction. I recently presented our paper on this topic, specifically on personalized dialog management at the Web Intelligence conference in Thessaloniki.</description>
    </item>
    
    <item>
      <title>How to Create a Music Bingo in Minutes</title>
      <link>https://florisdenhengst.github.io/posts/music-bingo/</link>
      <pubDate>Thu, 05 Sep 2019 15:39:37 +0200</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/music-bingo/</guid>
      <description>I was recently asked to help organise a &amp;lsquo;music bingo&amp;rsquo; for students of horseriding association BLOK. To be completely honest with you, I feel that bingo is probably one of the most boring party games out there as the only &amp;lsquo;skills&amp;rsquo; involved are paying attention and bookkeeping.
Music bingo, however, puts a nice twist to the original game that makes it more fun, exciting and skill-based. Music bingo is like regular bingo with some minor differences that make it just so much more fun.</description>
    </item>
    
    <item>
      <title>ACAI Summer School 2019</title>
      <link>https://florisdenhengst.github.io/posts/eurai-acai-2019/</link>
      <pubDate>Sat, 06 Jul 2019 16:41:29 +0200</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/eurai-acai-2019/</guid>
      <description>Just returned from a lovely stay in Chania, Crete for the Advanced Course on Artificial Intelligence, a yearly summer school by EurAI. Besides the beautiful scenery and lovely people, there were lots of interesting talks to enjoy. Most talks stuck to the theme &amp;lsquo;AI for multi-agent worlds&amp;rsquo; quite well.
Although I learned a lot in general, there were some key take-aways for me:
 virtually any setting has some multi-agent aspect to it, but it might not be worthwile to actually take it into account when going from a single- to a multi-agent setting, typically requires engineering and analysis of the resulting system from a game-theoretic point of view.</description>
    </item>
    
    <item>
      <title>Bitter Lesson Response</title>
      <link>https://florisdenhengst.github.io/posts/bitter-lesson-response/</link>
      <pubDate>Wed, 20 Mar 2019 23:27:28 +0100</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/bitter-lesson-response/</guid>
      <description>A couple of days ago, RL founding father Rich Sutton posted a blog post. My reading is as follows:
 Moore&amp;rsquo;s Law has consistently made general, compute-based methods outperform task-specific, domain knowledge-based methods for tasks in the AI `sphere of interest&#39; This has served as a bitter lesson for many (all?) researchers that have focused on developing such domain-specific methods General methods will always prevail, especially those that scale with compute  Although I tend to agree with the overall idea that general methods are the way forward for AI, I feel that some nuance is in order.</description>
    </item>
    
    <item>
      <title>How Reinforcement Learning is Applied to Dialogue Control</title>
      <link>https://florisdenhengst.github.io/posts/rl-for-dialog-management/</link>
      <pubDate>Fri, 15 Feb 2019 12:41:16 +0100</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/rl-for-dialog-management/</guid>
      <description>The value offering of most contemporary chatbot platforms consists of packaging state-of-art Automated Speech Recognition (ASR, or &amp;lsquo;speech-to-text&amp;rsquo;), Natural Language Understanding (NLU) and Voice Synthesis into a comprehensive API. The API typically also includes some programming model for dialog control such as DialogFlows&#39; Contexts and follow-up Intents and Alexa&amp;rsquo;s Dialog model. Implementing the right dialog controller is up to the developer. Figure 1 summarizes this in a diagram, with the handcrafted modules in green and with a keyboard in the top right.</description>
    </item>
    
    <item>
      <title>Research Methods in Information and Knowledge Systems Course</title>
      <link>https://florisdenhengst.github.io/posts/completed-research-methods-in-iks/</link>
      <pubDate>Sun, 30 Dec 2018 13:18:10 +0100</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/completed-research-methods-in-iks/</guid>
      <description>I completed the SIKS course &amp;lsquo;Research Methods and Methodology in IKS&amp;rsquo;, spanning a wide range of topics such as research methods, Design Research, many issues involved with evaluation in ML and how the field of IR has a strong history of rigourous evaluation (much to learn there for ML/RL researchers!).
To top it all off, the course was held in the lovely location (see picture) and with lots of great minds from all over The Netherlands.</description>
    </item>
    
    <item>
      <title>Scientific Writing Course</title>
      <link>https://florisdenhengst.github.io/posts/completed-scientific-writing-course/</link>
      <pubDate>Tue, 18 Dec 2018 13:10:34 +0100</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/completed-scientific-writing-course/</guid>
      <description>I just completed the course &amp;lsquo;Writing a Scientific Article&amp;rsquo; at the VU Language Centre. Although I joined the course thinking I knew a fair share of writing in English, I learned a great deal about scientific writing and picked up some neat tricks to improve writing. As a bonus, I got some great feedback on an survey paper on Reinforcement Learning for personalization I&amp;rsquo;m working on with some colleagues in our group</description>
    </item>
    
    <item>
      <title>Completed HPC Course</title>
      <link>https://florisdenhengst.github.io/posts/completed-hpc-course/</link>
      <pubDate>Thu, 29 Nov 2018 09:18:55 +0100</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/completed-hpc-course/</guid>
      <description>Today I completed the HPC course at VU University. The course was well organized and had some interesting courses. Some of the courses contained very little new information for me, though. I would recommend researchers/PhD. Students with some knowledge of programming/bash, Linux and clusters but with no experience working with SurfSARAs offerings to follow the following courses:
 Intro to distributed systems &amp;amp; BigData: optional refresher Intro to Linux and Clustercomputing: take, become familiar with Lisa Intro to MPI parallel programming concepts: take, get familiar with Cartesius Datamanagement: take to become familiar with various data storage/archiving solutions and make a data management plan for your research HPC Cloud: optional if interested in virtualization GPU Computing: as a refresher, somewhat theoretical Singularity with containerized applications: optional if interested in reproducibility using containers (e.</description>
    </item>
    
    <item>
      <title>The Book that Predicted AlphaGo</title>
      <link>https://florisdenhengst.github.io/posts/prediction-go-mcmc/</link>
      <pubDate>Wed, 01 Aug 2018 21:54:28 +0100</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/prediction-go-mcmc/</guid>
      <description>Little under three years ago, in March 2016, a Reinforcement Learning (RL) algorithm beat Lee Sedol, a pro player in a match of Go. This game had been considered too hard for algorithms due to the astronomical number of board game configurations and the AlphaGo team therefore baffled experts in AI with their accomplishments. Nobody seemed to be aware that techniques were available to make such a win possible at that time and most experts considered computers beating humanity at Go to be a couple of decades away.</description>
    </item>
    
  </channel>
</rss>
