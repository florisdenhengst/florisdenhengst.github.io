<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>The Book that Predicted AlphaGo | Floris den Hengst&#39;s Homepage</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
	<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML'
		    async></script>
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/publications/">Publications</a></li>
      
      <li><a href="/teaching/">Teaching</a></li>
      
      <li><a href="/posts/">Posts</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
    <h1><span class="title">The Book that Predicted AlphaGo</span></h1>
    
    <h2 class="date">2018/08/01</h2>
</div>

<main>
    <p>Little under three years ago, in March 2016, a Reinforcement Learning (RL) algorithm beat Lee
Sedol, a pro
player in a match of Go. This game had been considered too hard for algorithms due to the
<a href="https://en.wikipedia.org/wiki/Go_and_mathematics#Complexity_of_certain_Go_configurations">astronomical</a>
number of board game configurations and the AlphaGo team therefore baffled experts in AI with their
accomplishments. Nobody seemed to be aware that techniques were available to make such a win
possible at that time and most experts considered computers beating humanity at Go to be a couple
of decades away. Which is why I was surprised to find a mention of Go in a
&lsquo;Looking into the Future&rsquo; chapter of <a href="https://www.springer.com/us/book/9783642276446">a
book</a> on RL from 2012, which was
written at least four years prior to AlphaGo&rsquo;s triumph:</p>
<p>The quote reads:</p>
<pre tabindex="0"><code>Some very complex games, such as Go, are currently best played by a computer
that uses Monte-Carlo tree search (Coulom, 2006). RL methods can hardly be
employed well for playing such games.  The reason is that it is very difficult
to accurately represent and learn the value func- tion.  Therefore, a lot of
search and sampling currently works better than knowledge intensive solutions.
The search/knowledge tradeoff (Berliner, 1977) states that more knowledge will
come at the expense of less search, since models containing more parameters
consume more time to evaluate. It would be very interesting to gain un-
derstanding of novel RL methods that can quite accurately learn to approximate
the value function for complex games, but which are still very fast to evaluate.
Some research in this direction was performed for the game of chess (Baxter et
al, 1997; Veness et al, 2009) where linear models trained with RL could be
efficiently used by the search algorithms.
</code></pre><p>In the end, it turned out to be Deep Neural Networks instead of linear models. It took about four
years to get there, roughly equalling the lenght of a PhD in The Netherlands. It makes me wonder
which problems currently considered as &lsquo;unsolvable&rsquo; will be cracked in four years time.</p>

</main>


  <footer>
  <hr/>
  
  
  <a href="/index.xml"><img src='https://simpleicons.org/icons/rss.svg'></a>
  
  <a href="https://github.com/florisdenhengst/"><img src='https://simpleicons.org/icons/github.svg'></a>
  
  <a href="https://www.linkedin.com/in/floris-den-hengst-06ab7534/"><img src='https://simpleicons.org/icons/linkedin.svg'></a>
  
  <a href="https://orcid.org/0000-0002-2092-9904"><img src='/imgs/orcid.svg'></a>
  
  <a href="https://scholar.google.nl/citations?user=8I8iSHkAAAAJ"><img src='/imgs/gscholar.png'></a>
  
  | Created with <a href="https://gohugo.io">Hugo</a> and <a href="https://florisdenhengst.github.io/metis-example/">Metis</a> theme | Â© <a href="https://florisdenhengst.github.io">Floris den Hengst</a> 2018 &ndash; 2025
  
  </footer>
  </body>
</html>

