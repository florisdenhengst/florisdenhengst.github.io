<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>dialog systems on Floris den Hengst&#39;s Homepage</title>
    <link>https://florisdenhengst.github.io/tags/dialog-systems/</link>
    <description>Recent content in dialog systems on Floris den Hengst&#39;s Homepage</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Mar 2020 11:35:13 +0100</lastBuildDate>
    
	<atom:link href="https://florisdenhengst.github.io/tags/dialog-systems/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Collecting High Quality Ratings for Dialogue Systems</title>
      <link>https://florisdenhengst.github.io/posts/high-quality-ratings/</link>
      <pubDate>Fri, 20 Mar 2020 11:35:13 +0100</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/high-quality-ratings/</guid>
      <description>Mickey, a master student I was co-supervising, recently published a paper based on his thesis work in ACM&amp;rsquo;s Conference for Human Information Interaction and Retrieval. His work focused on interfaces for collecting high-quality user satisfaction ratings for dialogue systems.
User satisfaction is an important indicator in the design, evaluation and adaptation of these interfaces. Establishing user satisfaction ratings for conversation-based systems, however, remains challenging. User questionnaires may yield biased results and typically have low response rates (~1%).</description>
    </item>
    
    <item>
      <title>Personalized Dialogue Management</title>
      <link>https://florisdenhengst.github.io/posts/personalized-dm/</link>
      <pubDate>Mon, 21 Oct 2019 10:31:06 +0200</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/personalized-dm/</guid>
      <description>In a previous post I explained how reinforcement learning (RL) can be used to make chatbots better from its experience with users. RL allows chatbots to learn what to say by interacting with users. This allows for chatbots to tailor their behavior to preferences of groups of individuals, e.g. to personalize the interaction. I recently presented our paper on this topic, specifically on personalized dialog management at the Web Intelligence conference in Thessaloniki.</description>
    </item>
    
    <item>
      <title>How Reinforcement Learning is Applied to Dialogue Control</title>
      <link>https://florisdenhengst.github.io/posts/rl-for-dialog-management/</link>
      <pubDate>Fri, 15 Feb 2019 12:41:16 +0100</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/rl-for-dialog-management/</guid>
      <description>The value offering of most contemporary chatbot platforms consists of packaging state-of-art Automated Speech Recognition (ASR, or &amp;lsquo;speech-to-text&amp;rsquo;), Natural Language Understanding (NLU) and Voice Synthesis into a comprehensive API. The API typically also includes some programming model for dialog control such as DialogFlows&amp;rsquo; Contexts and follow-up Intents and Alexa&amp;rsquo;s Dialog model. Implementing the right dialog controller is up to the developer. Figure 1 summarizes this in a diagram, with the handcrafted modules in green and with a keyboard in the top right.</description>
    </item>
    
  </channel>
</rss>