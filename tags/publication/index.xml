<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>publication on Floris den Hengst&#39;s Homepage</title>
    <link>https://florisdenhengst.github.io/tags/publication/</link>
    <description>Recent content in publication on Floris den Hengst&#39;s Homepage</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Jun 2022 14:49:38 +0200</lastBuildDate><atom:link href="https://florisdenhengst.github.io/tags/publication/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Paper on strategic workforce planning with DRL at LOD</title>
      <link>https://florisdenhengst.github.io/posts/rl-for-swp-lod/</link>
      <pubDate>Tue, 14 Jun 2022 14:49:38 +0200</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/rl-for-swp-lod/</guid>
      <description>A paper on Deep Reinforcement Learning (DRL) for strategic workforce planning co-authored with Yannick Smit, Sandjai Bhulai and Ehsan Mehdad is accepted as a long paper at the LOD conference.
In this paper, we model strategic workforce planning as a stochastic nonlinear optimization problem, learn a generative model from data and use it as a simulator in a simulation-optimization approach.
We show that the DRL approach enables optimizing an organizations&amp;rsquo; strategic workforce goals directly.</description>
    </item>
    
    <item>
      <title>Option Machines paper accepted at IJCAI</title>
      <link>https://florisdenhengst.github.io/posts/option-machines-ijcai/</link>
      <pubDate>Sat, 21 May 2022 14:44:12 +0200</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/option-machines-ijcai/</guid>
      <description>The paper Reinforcement Learning with Option Machines co-authored by me, Vincent Fran√ßois-Lavet, Mark Hoogendoorn and Frank van Harmelen is accepted as a long presentation (~3% acceptance rate) at IJCAI.
Stay tuned for details!
Update 2022-08-01: I presented my paper at IJCAI, it was great fun!
Update 2022-09-23: The IJCAI proceedings are now available.</description>
    </item>
    
    <item>
      <title>Safe and Efficient Reinforcement Learning with Planning for Potential</title>
      <link>https://florisdenhengst.github.io/posts/planning-for-potential/</link>
      <pubDate>Thu, 24 Mar 2022 10:38:24 +0100</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/planning-for-potential/</guid>
      <description>Reinforcement Learning has proven to be capable of outperforming humans on various tasks by interacting with and experimenting some environment. This makes it one of the most interesting and promising AI solutions to problems that require complex behaviors which we are unable to fully define upfront but can assign an objective score to.
In many settings of interest such as in healthcare and finance, we&amp;rsquo;ll want to enforce safety constraints on any system of interest.</description>
    </item>
    
    <item>
      <title>Reinforcement Learning for Real Life Virtual Conference</title>
      <link>https://florisdenhengst.github.io/posts/rl-at-workshop/</link>
      <pubDate>Thu, 25 Jun 2020 21:14:18 +0200</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/rl-at-workshop/</guid>
      <description>Upcoming weekend will be a virtual conference on reinforcement learning (RL) in real-life.
It is of great interest to me primarily due to its program but also due to its organisation.
The virtual conference consists of two panel sessions and a virtual `poster&amp;rsquo; session with pre-recorded videos. There is a slack workspace for discussion, and questions to the panelists can be submitted up-front. Poster presenters are to host their own video channel.</description>
    </item>
    
    <item>
      <title>Reinforcement Learning for Personalization: A Survey</title>
      <link>https://florisdenhengst.github.io/posts/rl-for-pers-survey/</link>
      <pubDate>Thu, 09 Apr 2020 09:22:30 +0100</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/rl-for-pers-survey/</guid>
      <description>Update 2020-06-27: this paper was presented as a `poster&amp;rsquo;(video) at the RL for Real Life virtual Conference. Read more here.
Reinforcement learning (RL) is becoming an increasingly popular tool to tackle hairy problems using data. A nice example of such a hairy problem is personalization. Personalization refers to a task central to many applications of data science and machine learning: to change a system so that its personal relevance to an individual or category of individuals is increased.</description>
    </item>
    
    <item>
      <title>Collecting User Satisfaction Ratings for Dialogue Systems</title>
      <link>https://florisdenhengst.github.io/posts/high-quality-ratings/</link>
      <pubDate>Fri, 20 Mar 2020 11:35:13 +0100</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/high-quality-ratings/</guid>
      <description>Mickey, a master student I was co-supervising, recently published a paper based on his thesis work in ACM&amp;rsquo;s Conference for Human Information Interaction and Retrieval. His work focused on interfaces for collecting high-quality user satisfaction ratings for dialogue systems.
User satisfaction is an important indicator in the design, evaluation and adaptation of dialogue systems. Establishing user satisfaction ratings for conversation-based systems, however, remains challenging. User questionnaires may yield biased results and typically have low response rates (~1%).</description>
    </item>
    
    <item>
      <title>Personalized Dialogue Management</title>
      <link>https://florisdenhengst.github.io/posts/personalized-dm/</link>
      <pubDate>Mon, 21 Oct 2019 10:31:06 +0200</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/personalized-dm/</guid>
      <description>In a previous post I explained how reinforcement learning (RL) can be used to make chatbots better from its experience with users. RL allows chatbots to learn what to say by interacting with users. This allows for chatbots to tailor their behavior to preferences of groups of individuals, e.g. to personalize the interaction. I recently presented our paper on this topic, specifically on personalized dialog management at the Web Intelligence conference in Thessaloniki.</description>
    </item>
    
  </channel>
</rss>
