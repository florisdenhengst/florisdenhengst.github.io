<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>reinforcement learning on Floris den Hengst&#39;s Homepage</title>
    <link>https://florisdenhengst.github.io/tags/reinforcement-learning/</link>
    <description>Recent content in reinforcement learning on Floris den Hengst&#39;s Homepage</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 01 Aug 2022 16:43:48 +0200</lastBuildDate><atom:link href="https://florisdenhengst.github.io/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>IJCAI 2022</title>
      <link>https://florisdenhengst.github.io/posts/ijcai2022/</link>
      <pubDate>Mon, 01 Aug 2022 16:43:48 +0200</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/ijcai2022/</guid>
      <description>Last week I attended the International Joint Conference on Artificial Intelligence (IJCAI) in Vienna. Being `co-hosted&#39; with the European Conference on AI, the official name of this years&#39; edition was IJCAI-ECAI 2022. It was a great week with too many impressions and learnings to list. Here are some personal highlights:
 presented our paper on Option Machines as a long talk in the `Deep Reinforcement Learning&#39; session presented our paper on Safe &amp;amp; Efficient RL with Planning for Potential at the Safe RL workshop met many great people with great humor, ideas and experience attended keynotes by two Giants in AI: Stuart Russel and Judea Pearl had a nice visit of the Vienna.</description>
    </item>
    
    <item>
      <title>Reinforcement Learning Summer School 2022</title>
      <link>https://florisdenhengst.github.io/posts/reinforcement-learning-summer-school/</link>
      <pubDate>Sat, 16 Jul 2022 10:27:22 +0200</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/reinforcement-learning-summer-school/</guid>
      <description>Past week, I attended and assisted at the Reinforcement Learning Summer School organized by Vincent François-Lavet. We had some great lectures on a wide variety of topics, including pure exploration in bandits, the exploration-exploitation tradeoff in RL, MCTS, symmetries and state similarities, world models and hierarchical and distributational RL.
The summer school was hosted at one the theatre halls of our university&amp;rsquo;s new building, which serves as a movie theatre at night: great seats and air quality do help with focusing for 8+ hours!</description>
    </item>
    
    <item>
      <title>A Brief intro to Gaussian Processes</title>
      <link>https://florisdenhengst.github.io/posts/intro-to-gps/</link>
      <pubDate>Tue, 21 Jun 2022 14:12:24 +0200</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/intro-to-gps/</guid>
      <description>Gaussian Processes are a fascinating tool for usage in RL due to modelling uncertainty and data efficiency I briefly introduced GP&amp;rsquo;s and shown how/why they are used in RL  Gaussian processes (GPs) are a fascinating tool in the machine learning toolbelt. They stand out for a couple of reasons: some people will like them for their data efficiency, others love them for their ability to incorporate domain knowledge and yet others will love them for their visual or mathematical beauty.</description>
    </item>
    
    <item>
      <title>Paper on strategic workforce planning with DRL at LOD</title>
      <link>https://florisdenhengst.github.io/posts/rl-for-swp-lod/</link>
      <pubDate>Tue, 14 Jun 2022 14:49:38 +0200</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/rl-for-swp-lod/</guid>
      <description>A paper on Deep Reinforcement Learning (DRL) for strategic workforce planning co-authored with Yannick Smit, Sandjai Bhulai and Ehsan Mehdad is accepted as a long paper at the LOD conference.
In this paper, we model strategic workforce planning as a stochastic nonlinear optimization problem, learn a generative model from data and use it as a simulator in a simulation-optimization approach.
We show that the DRL approach enables optimizing an organizations&#39; strategic workforce goals directly.</description>
    </item>
    
    <item>
      <title>Option Machines paper accepted at IJCAI</title>
      <link>https://florisdenhengst.github.io/posts/option-machines-ijcai/</link>
      <pubDate>Sat, 21 May 2022 14:44:12 +0200</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/option-machines-ijcai/</guid>
      <description>The paper Reinforcement Learning with Option Machines co-authored by me, Vincent François-Lavet, Mark Hoogendoorn and Frank van Harmelen is accepted as a long presentation (~3% acceptance rate) at IJCAI.
Stay tuned for details!</description>
    </item>
    
    <item>
      <title>Safe and Efficient Reinforcement Learning with Planning for Potential</title>
      <link>https://florisdenhengst.github.io/posts/planning-for-potential/</link>
      <pubDate>Thu, 24 Mar 2022 10:38:24 +0100</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/planning-for-potential/</guid>
      <description>Reinforcement Learning has proven to be capable of outperforming humans on various tasks by interacting with and experimenting some environment. This makes it one of the most interesting and promising AI solutions to problems that require complex behaviors which we are unable to fully define upfront but can assign an objective score to.
In many settings of interest such as in healthcare and finance, we&amp;rsquo;ll want to enforce safety constraints on any system of interest.</description>
    </item>
    
    <item>
      <title>Reinforcement Learning for Real Life Virtual Conference</title>
      <link>https://florisdenhengst.github.io/posts/rl-at-workshop/</link>
      <pubDate>Thu, 25 Jun 2020 21:14:18 +0200</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/rl-at-workshop/</guid>
      <description>Upcoming weekend will be a virtual conference on reinforcement learning (RL) in real-life.
It is of great interest to me primarily due to its program but also due to its organisation.
The virtual conference consists of two panel sessions and a virtual `poster&#39; session with pre-recorded videos. There is a slack workspace for discussion, and questions to the panelists can be submitted up-front. Poster presenters are to host their own video channel.</description>
    </item>
    
    <item>
      <title>Reinforcement Learning for Personalization: A Survey</title>
      <link>https://florisdenhengst.github.io/posts/rl-for-pers-survey/</link>
      <pubDate>Thu, 09 Apr 2020 09:22:30 +0100</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/rl-for-pers-survey/</guid>
      <description>Update 2020-06-27: this paper was presented as a `poster&#39;(video) at the RL for Real Life virtual Conference. Read more here.
Reinforcement learning (RL) is becoming an increasingly popular tool to tackle hairy problems using data. A nice example of such a hairy problem is personalization. Personalization refers to a task central to many applications of data science and machine learning: to change a system so that its personal relevance to an individual or category of individuals is increased.</description>
    </item>
    
    <item>
      <title>Personalized Dialogue Management</title>
      <link>https://florisdenhengst.github.io/posts/personalized-dm/</link>
      <pubDate>Mon, 21 Oct 2019 10:31:06 +0200</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/personalized-dm/</guid>
      <description>In a previous post I explained how reinforcement learning (RL) can be used to make chatbots better from its experience with users. RL allows chatbots to learn what to say by interacting with users. This allows for chatbots to tailor their behavior to preferences of groups of individuals, e.g. to personalize the interaction. I recently presented our paper on this topic, specifically on personalized dialog management at the Web Intelligence conference in Thessaloniki.</description>
    </item>
    
    <item>
      <title>Bitter Lesson Response</title>
      <link>https://florisdenhengst.github.io/posts/bitter-lesson-response/</link>
      <pubDate>Wed, 20 Mar 2019 23:27:28 +0100</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/bitter-lesson-response/</guid>
      <description>A couple of days ago, RL founding father Rich Sutton posted a blog post. My reading is as follows:
 Moore&amp;rsquo;s Law has consistently made general, compute-based methods outperform task-specific, domain knowledge-based methods for tasks in the AI `sphere of interest&#39; This has served as a bitter lesson for many (all?) researchers that have focused on developing such domain-specific methods General methods will always prevail, especially those that scale with compute  Although I tend to agree with the overall idea that general methods are the way forward for AI, I feel that some nuance is in order.</description>
    </item>
    
    <item>
      <title>How Reinforcement Learning is Applied to Dialogue Control</title>
      <link>https://florisdenhengst.github.io/posts/rl-for-dialog-management/</link>
      <pubDate>Fri, 15 Feb 2019 12:41:16 +0100</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/rl-for-dialog-management/</guid>
      <description>The value offering of most contemporary chatbot platforms consists of packaging state-of-art Automated Speech Recognition (ASR, or &amp;lsquo;speech-to-text&amp;rsquo;), Natural Language Understanding (NLU) and Voice Synthesis into a comprehensive API. The API typically also includes some programming model for dialog control such as DialogFlows&#39; Contexts and follow-up Intents and Alexa&amp;rsquo;s Dialog model. Implementing the right dialog controller is up to the developer. Figure 1 summarizes this in a diagram, with the handcrafted modules in green and with a keyboard in the top right.</description>
    </item>
    
    <item>
      <title>Scientific Writing Course</title>
      <link>https://florisdenhengst.github.io/posts/completed-scientific-writing-course/</link>
      <pubDate>Tue, 18 Dec 2018 13:10:34 +0100</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/completed-scientific-writing-course/</guid>
      <description>I just completed the course &amp;lsquo;Writing a Scientific Article&amp;rsquo; at the VU Language Centre. Although I joined the course thinking I knew a fair share of writing in English, I learned a great deal about scientific writing and picked up some neat tricks to improve writing. As a bonus, I got some great feedback on an survey paper on Reinforcement Learning for personalization I&amp;rsquo;m working on with some colleagues in our group</description>
    </item>
    
    <item>
      <title>The Book that Predicted AlphaGo</title>
      <link>https://florisdenhengst.github.io/posts/prediction-go-mcmc/</link>
      <pubDate>Wed, 01 Aug 2018 21:54:28 +0100</pubDate>
      
      <guid>https://florisdenhengst.github.io/posts/prediction-go-mcmc/</guid>
      <description>Little under three years ago, in March 2016, a Reinforcement Learning (RL) algorithm beat Lee Sedol, a pro player in a match of Go. This game had been considered too hard for algorithms due to the astronomical number of board game configurations and the AlphaGo team therefore baffled experts in AI with their accomplishments. Nobody seemed to be aware that techniques were available to make such a win possible at that time and most experts considered computers beating humanity at Go to be a couple of decades away.</description>
    </item>
    
  </channel>
</rss>
